{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import packages\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import lightgbm\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\royal\\\\Downloads\\\\Compressed\\\\data\\\\Toxic Comment chal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "f0bcd6ca-a054-46de-a734-6b06bcb84761",
    "_uuid": "0a45c8a0d39df7464fdd2c9c9f9d3fb972728881",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "7228e404-6568-4857-97af-d078ff59ce86",
    "_uuid": "747b0afaac5c32c5bf92c62060bfd435dc62ad10",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "# Tweet tokenizer does not split at apostophes which is what we want\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "46dad85c-d795-4228-afea-de2c1172b93c",
    "_uuid": "62e990af1d81c05a3c0399a1871220769100fe6c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer=TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "subm = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "f04ae24d-0f3f-4493-8f93-4e3c70342602",
    "_uuid": "9ac357cb386ece84c44b325cf83cff423f936371",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#merge into single pandas object\n",
    "merge=pd.concat([train.iloc[:,0:2],test.iloc[:,0:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "5c9caca0-b68d-44f4-91db-5e7c383314bf",
    "_uuid": "225a0a9babfb824ee11dd5874911dfeed6296736",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus=merge['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "c46e7feb0060d433b462ed50cbd7cfa3e8b65856",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = corpus.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e866d1da-9482-4bd0-970b-76a6e6a1f36e",
    "_uuid": "9de121ab9d805fd1901526020fac3d84bdc410e4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "319a64d4-47c7-417b-90d1-190bcf0b42ea",
    "_uuid": "6fcd1656649474cb2d7d46ff1a377a29bef99d03",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace the Appostrophes,smileys and some commonly misspelled words.\n",
    "APPO = {\n",
    "\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"i would\",\n",
    "\"i'd\" : \"i had\",\n",
    "\"i'll\" : \"i will\",\n",
    "\"i'm\" : \"i am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"i have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"\n",
    "}\n",
    "\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    r\"\\br\\b\": \"are\",\n",
    "    r\"\\bu\\b\": \"you\",\n",
    "    r\"\\bhaha\\b\": \"ha\",\n",
    "    r\"\\bhahaha\\b\": \"ha\",\n",
    "    r\"\\bdon't\\b\": \"do not\",\n",
    "    r\"\\bdoesn't\\b\": \"does not\",\n",
    "    r\"\\bdidn't\\b\": \"did not\",\n",
    "    r\"\\bhasn't\\b\": \"has not\",\n",
    "    r\"\\bhaven't\\b\": \"have not\",\n",
    "    r\"\\bhadn't\\b\": \"had not\",\n",
    "    r\"\\bwon't\\b\": \"will not\",\n",
    "    r\"\\bwouldn't\\b\": \"would not\",\n",
    "    r\"\\bcan't\\b\": \"can not\",\n",
    "    r\"\\bcannot\\b\": \"can not\",\n",
    "    r\"\\bi'm\\b\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"r\": \"are\",\n",
    "    \"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"can't\": \"can not\",\n",
    "    \"cannot\": \"can not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"m\": \"am\",\n",
    "    \"i'll\" : \"i will\",\n",
    "    \"its\" : \"it is\",\n",
    "    \"it's\" : \"it is\",\n",
    "    \"'s\" : \" is\",\n",
    "    \"that's\" : \"that is\",\n",
    "    \"weren't\" : \"were not\",\n",
    "    \"wtf\":\"what the fuck\",\n",
    "    \"f ck\":\"fuck\",\n",
    "    \"fucka\":\"fuck\",\n",
    "    \"sh t\":\"shit\",\n",
    "    \"fucky\":\"fuck\",\n",
    "    \"mothafuc\":\"mothafucker\",\n",
    "    \"fucksex\":\"fuck sex\",\n",
    "    \"yourselfgo\":\"yourself go\",\n",
    "    \"criminalwar\":\"criminal war\",\n",
    "    \"securityfuck\":\"security fuck\",\n",
    "    \"ancestryfuck\":\"ancestry fuck\",\n",
    "    \"phuq\":\"fuck\",\n",
    "    \"slimvirgin\":\"slim virgin\",\n",
    "    \"itsuck\":\"it suck\",\n",
    "    \"talk2me\":\"talk to me\",\n",
    "    \"fuckingabf\":\"fucking\",\n",
    "    \"fuckbags\":\"fuck bags\",\n",
    "    \"failepic\":\"fail epic\",\n",
    "    'a n a l': 'anal',\n",
    " 'a n u s': 'anus',\n",
    " 'a r s e': 'arse',\n",
    " 'a s s': 'ass',\n",
    " 'animalfucker': 'animal fucker',\n",
    " 'assclowns': 'ass clowns',\n",
    " 'assface': 'ass face',\n",
    " 'asskicked': 'ass kicked',\n",
    " 'asswhipe': 'ass whipe',\n",
    " 'b a l l s': 'balls',\n",
    " 'b a l l s a c k': 'ballsack',\n",
    " 'b a s t a r d': 'bastard',\n",
    " 'b i a t c h': 'bitch',\n",
    " 'b i t c h': 'bitch',\n",
    " 'b l o o d y': 'bloody',\n",
    " 'b l o w j o b': 'blowjob',\n",
    " 'b o l l o c k': 'bollock',\n",
    " 'b o l l o k': 'bollok',\n",
    " 'b o n e r': 'boner',\n",
    " 'b o o b': 'boob',\n",
    " 'b u g g e r': 'bugger',\n",
    " 'b u m': 'bum',\n",
    " 'b u t t': 'butt',\n",
    " 'b u t t p l u g': 'buttplug',\n",
    " 'badassness': 'bad ass',\n",
    " 'bicth': 'bitch',\n",
    " 'bitchass': 'bitch ass',\n",
    " 'bitchmattythewhite': 'bitch matty the white',\n",
    " 'bitchmother': 'bitch mother',\n",
    " 'bitchs': 'bitch',\n",
    " 'blowjob': 'blow job',\n",
    " 'boymamas': 'boy mamas',\n",
    " 'c l i t o r i s': 'clitoris',\n",
    " 'c o c k': 'cock',\n",
    " 'c o o n': 'coon',\n",
    " 'c r a p': 'crap',\n",
    " 'c u n t': 'cunt',\n",
    " 'corpsefucking': 'corpse fucking',\n",
    " 'cuntbag': 'cunt bag',\n",
    " 'cuntface': 'cunt face',\n",
    " 'cuntfranks': 'cunt franks',\n",
    " 'cuntliz': 'cunt liz',\n",
    " 'd a m n': 'damn',\n",
    " 'd i c k': 'dick',\n",
    " 'd i l d o': 'dildo',\n",
    " 'dickbag': 'dick bag',\n",
    " 'dickbig': 'dick big',\n",
    " 'dickbreath': 'dick breath',\n",
    " 'dickbutt': 'dick butt',\n",
    " 'dickheaditalic': 'dick head italic',\n",
    " 'f a g': 'fag',\n",
    " 'f e c k': 'feck',\n",
    " 'f e l c h i n g': 'felching',\n",
    " 'f e l l a t e': 'fellate',\n",
    " 'f e l l a t i o': 'fellatio',\n",
    " 'f l a n g e': 'flange',\n",
    " 'f u c k': 'fuck',\n",
    " 'f u d g e p a c k e r': 'fudge packer',\n",
    " 'failepic': 'fail epic',\n",
    " 'fatmansanger': 'fat mananger',\n",
    " 'fcken': 'fuck',\n",
    " 'fckin': 'fuck',\n",
    " 'fcking': 'fuck',\n",
    " 'fking': 'fuck',\n",
    " 'fonkin': 'fuck',\n",
    " 'fuck': 'fuck',\n",
    " 'fuckan': 'fucking',\n",
    " 'fuckass': 'fuck ass',\n",
    " 'fuckbags': 'fuck bags',\n",
    " 'fuckedy': 'fuck',\n",
    " 'fuckhole': 'fuck hole',\n",
    " 'fuckiest': 'fuck',\n",
    " 'fuckign': 'fuck',\n",
    " 'fuckingabf': 'fuck',\n",
    " 'fuckk': 'fuck',\n",
    " 'fuckon': 'fuck',\n",
    " 'fucksex': 'fuck sex',\n",
    " 'fuckstick': 'fuck stick',\n",
    " 'fuckwads': 'fuck',\n",
    " 'fuckyourself': 'fuck yourself',\n",
    " 'fudgepacker': 'fudge packer',\n",
    " 'fukcing': 'fuck',\n",
    " 'fuked': 'fuck',\n",
    " 'fuking': 'fuck',\n",
    " 'fukkers': 'fuck',\n",
    " 'fukyou': 'fuck you',\n",
    " 'g o d d a m n': 'god damn',\n",
    " 'goddamn': 'god damn',\n",
    " 'h e l l': 'hell',\n",
    " 'h o m o': 'homo',\n",
    " 'i d i o t': 'idiot',\n",
    " 'itiot': 'idiot',\n",
    " 'itsuck': 'it suck',\n",
    " 'j e r k': 'jerk',\n",
    " 'j i z z': 'jizz',\n",
    " 'jpgsuck': 'jpg suck',\n",
    " 'k n o b e n d': 'knob end',\n",
    " 'kickwars': 'kick wars',\n",
    " 'knobend': 'knob end',\n",
    " 'l a b i a': 'labia',\n",
    " 'l m a o': 'lmao',\n",
    " 'l m f a o': 'lmfao',\n",
    " 'm u f f': 'muff',\n",
    " 'marcolfuck': 'marcol fuck',\n",
    " 'mothafuckin': 'mother fuck',\n",
    " 'motherfu': 'mother fuck',\n",
    " 'n i g g a': 'nigga',\n",
    " 'n i g g e r': 'nigger',\n",
    " 'niggertard': 'nigger tard',\n",
    " 'o m g': 'omg',\n",
    " 'ofuck': 'oh fuck',\n",
    " 'oldlady': 'old lady',\n",
    " 'p e n i s': 'penis',\n",
    " 'p i s s': 'piss',\n",
    " 'p o o p': 'poop',\n",
    " 'p r i c k': 'prick',\n",
    " 'p u b e': 'pube',\n",
    " 'p u s s y': 'pussy',\n",
    " 'penistown': 'penis',\n",
    " 'pensnsnniensnsn': 'penis',\n",
    " 'pneis': 'penis',\n",
    " 'popsucker': 'pop sucker',\n",
    " 'q u e e r': 'queer',\n",
    " 's c r o t u m': 'scrotum',\n",
    " 's e x': 'sex',\n",
    " 's h 1 t': 'shit',\n",
    " 's h i t': 'shit',\n",
    " 's l u t': 'slut',\n",
    " 's m e g m a': 'smegma',\n",
    " 's p u n k': 'spunk',\n",
    " 's u c k': 'suck',\n",
    " 'sexcual': 'sexual',\n",
    " 'sexsex': 'sex',\n",
    " 'shiot': 'shit',\n",
    " 'shioty': 'shit',\n",
    " 'shitdip': 'shit',\n",
    " 'shitfuck': 'shit fuck',\n",
    " 'shitler': 'shit',\n",
    " 'shitlol': 'shit',\n",
    " 'shitush': 'shit',\n",
    " 'shoit': 'shit',\n",
    " 'suckdickeer': 'suck dick',\n",
    " 'suckersyou': 'suck you',\n",
    " 'suckipedia': 'suck',\n",
    " 'suckish': 'suck',\n",
    " 'sucksfrozen': 'suck frozen',\n",
    " 't i t': 'tit',\n",
    " 't o s s e r': 'tosser',\n",
    " 't u r d': 'turd',\n",
    " 't w a t': 'twat',\n",
    " 'v a g i n a': 'vagina',\n",
    " 'vbutt': 'butt',\n",
    " 'w a n k': 'wank',\n",
    " 'w a n k e r': 'wanker',\n",
    " 'w h o r e': 'whore',\n",
    " 'w t f': 'wtf',\n",
    " 'wikifuckers': 'fuckers',\n",
    " 'wikihomosexuals': 'homosexual',\n",
    " 'wikijews': 'jew',\n",
    " 'wikipedidiots': 'idiot',\n",
    " 'wikipedophiles': 'pedophile',\n",
    " 'wikiretards': 'retard',\n",
    " 'wikisucks': 'suck',\n",
    " 'wikitheclown': 'clown',\n",
    " 'wikiwankers': 'wanker'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "1a17cfd7-d7bd-463e-8e5d-851ca71fc13a",
    "_uuid": "9e0ea697e04c869d3036ad68764da57c2a579de2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replkeys = [i for i in repl.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clearup(s, chars):\n",
    "    return re.sub('[%s]' % chars, '', s).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_cell_guid": "b433a164-2b06-46f1-adce-c9eeea84d809",
    "_uuid": "45605350d49f62f3501edd2fb4fef9c67c44ad53",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove the punctuation from the text.\n",
    "punctu=['\"', '%', '&', \"'\", '(', ')', '+', '-', '/', ':', ';', '<', '=', '>', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in punctu:\n",
    "        text = text.replace(punctuation, ' ')\n",
    "        text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "_cell_guid": "3a551329-aeb9-491b-9617-2919a411d79b",
    "_uuid": "1c47e57d3ce0427464c43c1591f529c442cf7f80",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function receives comments and returns clean word-list\n",
    "\"\"\"\n",
    "def clean(comment):\n",
    "    #Convert to lower case , so that Hi and hi are the same\n",
    "    #comment=comment.lower()\n",
    "    #remove \\n\n",
    "    comment=re.sub(\"\\\\n\",\" \",comment)\n",
    "    # remove leaky elements like ip,user\n",
    "    comment=re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",\" \",comment)\n",
    "    #removing usernames\n",
    "    comment=re.sub(\"\\[\\[.*\\]\",\" \",comment)\n",
    "    #remove hyperlink\n",
    "    comment=re.sub(\"http\\S+|www.\\S+\",\" \",comment)\n",
    "    #remove numbers\n",
    "    comment = clearup(comment, string.punctuation+string.digits)\n",
    "    #Split the sentences into words\n",
    "    words=tokenizer.tokenize(comment)\n",
    "    # (')aphostophe  replacement (ie)   you're --> you are  \n",
    "    # ( basic dictionary lookup : master dictionary present in a hidden block of code)\n",
    "    words=[APPO[word] if word in APPO else word for word in words]\n",
    "    #other commonly misspeled words.\n",
    "    words=[repl[word] if word in replkeys else word for word in words]\n",
    "    clean_sent=\" \".join(words)\n",
    "    # remove any non alphanum,digit character\n",
    "    clean_sent=re.sub(\"\\W+\",\" \",clean_sent)\n",
    "    # remove the punctuations from the text\n",
    "    remove_punctuations(clean_sent)\n",
    "    clean_sent=re.sub(\"  \",\" \",clean_sent)\n",
    "    clean_sent=clean_sent.strip()\n",
    "    return(clean_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45        i don't believe the lisak criticism present th...\n",
       "73        \"\\n ambiguous ? \\nso mabuska irish can mean mo...\n",
       "193       about mitch moved to yggdrasill \\n\\ni have no ...\n",
       "249       \"\\nit's in the history, or more conveniently i...\n",
       "404       hi collectonian, it obviously wasn't appropria...\n",
       "454       \"\\ni want to offer you a tremendous \"\"thank yo...\n",
       "484       \"== tfd nomination of template:silentredirect ...\n",
       "492       it wasn't me that added the glebe park link, t...\n",
       "538       i'm disappointed it wasn't a successful run, b...\n",
       "553       ipad talk page \\n\\nwhy did you remove that ip'...\n",
       "557       \"\\n\\nnope, i didn't read the guide to appealin...\n",
       "791       referring to her as *catherine* \\n\\ni was told...\n",
       "955       \":you said: \"\"i am determined that the page wi...\n",
       "1051      thanks to all for taking my rather offhand com...\n",
       "1130      ha! \\n\\nthere wasn't a blp violation in the co...\n",
       "1164             your edit summary wasn't very nice either.\n",
       "1231      \"\\n\\n hello \\n\\nhi blu aardvark, glad to see y...\n",
       "1278      current events story\\nhi, i removed the story ...\n",
       "1379      \"i'm back. i already created  id (occasionally...\n",
       "1481      \"\\n\\npete, what makes you think you can go aro...\n",
       "1492      external links \\n\\ni've added the official (bo...\n",
       "1512      , 1 december 2010 (utc)\\n\\n the current rules ...\n",
       "1591      \"\\n\\n \\n  has hugged you! hugs promote wikilov...\n",
       "1799      no problem.  i was confused by the see also be...\n",
       "1916      \"\\nno it wasn't scrapped. alicia's version rem...\n",
       "1955      \"\\n\\nfriend, no problem with me. but just look...\n",
       "1964      \"\\n\\n misleading reference to study? \\n\\nin th...\n",
       "2075      um, sure ecw initials mean something, but they...\n",
       "2208      \"\\n\\n\"\"the wiki foundation stats, in an email ...\n",
       "2218      thanks!  i wasn't sure how to list dennis bowm...\n",
       "                                ...                        \n",
       "150826    ::::maybe you should just take a deep breath a...\n",
       "150878    \"regarding the name... first off, in the origi...\n",
       "150924    \" \\n\\n  \\n ==prisoners section, note to sarast...\n",
       "150975    == doppelgÃ¤nger blocked == \\n\\n i just saw tha...\n",
       "151019    ru pravda== \\n dear irpen! sorry if i chose th...\n",
       "151044    \" \\n\\n == your edit == \\n\\n here  you wrote th...\n",
       "151194    \" \\n\\n sooo.. i think i've gone as far as i ca...\n",
       "151326    \" \\n\\n ::::::::::hi pilcha...i didn't understa...\n",
       "151350    ===merge=== \\n  why does this song have an art...\n",
       "151382    \" \\n\\n == do they know its christmas? == \\n\\n ...\n",
       "151503        president lincoln was stupid and wasn't nice.\n",
       "151692    \"=0&map;=map&currenttab;=tabs-1&pagedate;=11%2...\n",
       "151726    ==the sibley guide to bird life and behavior==...\n",
       "152024    ::but he was accused of crimes unrelated to a ...\n",
       "152200    \"{| style=\"\"float:right;\"\" \\n | \\n | \\n |} \\n\\...\n",
       "152251    \"== a barnstar for you! == \\n\\n {| style=\"\"bac...\n",
       "152253    \" \\n :i've added content to it at some point. ...\n",
       "152384    * alison! where the hell have you been? i was ...\n",
       "152478    :::that is an odd reply. if you did research, ...\n",
       "152497    \" \\n ::like gloss, inediblehulk wasn't as invo...\n",
       "152688    \" \\n :from what i saw of the place, activity t...\n",
       "152693    \":::::*the way the usa defines what is and wha...\n",
       "152701    as i have told abigail at her talk, no it wasn...\n",
       "152767    == james dignan == \\n\\n hi de piep - you wrote...\n",
       "152807    \" \\n :::::::::::i don't recall writing that i ...\n",
       "152894    :no problem. i just wasn't certain if it was t...\n",
       "153012    ::: firstly, i find the nitpicking ridiculous....\n",
       "153015    \" \\n\\n ::::just to be clear, i'm not taking is...\n",
       "153022    \" \\n :sorry if i wasn't clear. my concern was ...\n",
       "153134    \" \\n\\n == same coffee shop? == \\n\\n my memory ...\n",
       "Name: comment_text, Length: 3578, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus.iloc[corpus.str.contains('suckish')]\n",
    "corpus[corpus.str.contains(\"wasn't\")]\n",
    "#.str.contains('Mel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#apply the clean method on first 200 comments\n",
    "clean_corpus200=corpus[:200].apply(lambda x :clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "_cell_guid": "56a0dca3-b2dd-4693-bab9-7159b343b6b9",
    "_uuid": "b745586afcab4bf3a6dc2f55751e68435a2e2f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clean_corpus=corpus.apply(lambda x :clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6346      riaa certifications someone got that shit fuck...\n",
       "14025     hola fucker retard who has nothing better to d...\n",
       "15469     whatthefuck what the fuck dude i cant add my o...\n",
       "18199                   you better fuck off ruddy shit fuck\n",
       "30737     hey you know what fuck this it takes to goddam...\n",
       "31680     nuisance piece of shit fuck off wiki doesnt ne...\n",
       "32814     just this account dipshit fuck are you dumb a ...\n",
       "36213     i feel the page should not be deleted as you g...\n",
       "40597     fuck you you piece of shit fuck you you piece ...\n",
       "40941                   you better fuck off ruddy shit fuck\n",
       "41390     she is a motherfucker i will block you motherf...\n",
       "42919     eat shit fucko suck my asshole then die horrib...\n",
       "52456     chestop adding in shit only to support your fu...\n",
       "61304        you cant delete it siece of shit fuck your mom\n",
       "62768     suck it bitch suck my bigass cock gag on it mo...\n",
       "66463     fuck you niggerkite y you deleting so much shi...\n",
       "71866     i will find you in real life and kill you i wi...\n",
       "71977     go fuck yourself too you fucking piece of shit...\n",
       "80876     shit shit fuck you random moi youre son of a b...\n",
       "81764                       nuisance piece of shit fuck off\n",
       "83827     oh no i just read the vile diatribe you left f...\n",
       "86419     douche bag douche bag douche bag douche bag fu...\n",
       "86809     is something wrong with you you fucking freak ...\n",
       "88173     youre so fucking ugly are you walters bitch he...\n",
       "100222    urgent mind yo own fucken buisiness stop fucke...\n",
       "106017    piece of shit fuck your warning and fuck your ...\n",
       "109801                    hi and fuck you eat shit fucktard\n",
       "118012    andrew quah is dogshit fuck girraween hs you m...\n",
       "121626    audi a you are asshole motherfucker shit fuck ...\n",
       "123794    she is a motherfucker i will block you motherf...\n",
       "                                ...                        \n",
       "78919                                 shit fucking blows up\n",
       "79853     i havent decided yet are you going to block th...\n",
       "82658     wow theres no fucking differrence between a br...\n",
       "84542     fuck you azzholeseat shit fuck you in the man ...\n",
       "88849                          dont delete my shit fuck off\n",
       "89145                        wikipedia is bulshit fuck them\n",
       "90118     wikipedia aint censored cover your childrens e...\n",
       "98341     thanks for ruining another phish page greatway...\n",
       "98908     juggalo life this is one of my favoit songs by...\n",
       "99486     i would jus lke say thz site is all bullshit c...\n",
       "100502    shit is very common and not very offensive jus...\n",
       "103078    no fuck the fuckik shit fuckassno shit fuck ho...\n",
       "119156                          clue bot eats shit fuck you\n",
       "121616    hi i figured id ask you because im pretty unsu...\n",
       "123063    fuck shit whore putting swear words on the int...\n",
       "124358    all wikipedia admins are fucking faggots and c...\n",
       "132075    fuck you if people cant write whatever they wa...\n",
       "134786    this is bullshit fuck the world and this mothe...\n",
       "135375    all wikipedia admins are fucking faggots and c...\n",
       "135689    you finally proved it once and for all wikiped...\n",
       "136939           shit fuck me i like mennnnnnnnnnnnnnnnnnnn\n",
       "137950                            fucking shit fucker bitch\n",
       "139095    expletive bad language shit fuck piss you must...\n",
       "146815    well i dont give a fuck what you think you bit...\n",
       "147914    shoveling horse shit when i was when i was i w...\n",
       "150716    fuck you motherfucker piece of shit fuck you m...\n",
       "150797    shit fuck article i resent having my article c...\n",
       "151831    only bros like this shit fuck dane cook every ...\n",
       "151911    bold textfuck you ass hole then yah fuck you y...\n",
       "152090    go shove a fucking flaming pitchfork up your c...\n",
       "Name: comment_text, Length: 102, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus[clean_corpus.str.contains(\"shit fuck\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"thanks, josette. i enjoyed meeting you, too. i was shocked by the decision, which does not begin to reflect consensus. does just one grand poobah make it alone? serves me right for stealing time from more pressing real-world duties to indulge in a fun hobby. i've learned my lesson and won't waste time like that again. i'll stick to fixing the little things i run across as i read articles for my own information.\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.iloc[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks josette i enjoyed meeting you too i was shocked by the decision which does not begin to reflect consensus does just one grand poobah make it alone serves me right for stealing time from more pressing realworld duties to indulge in a fun hobby ive learned my lesson and wont waste time like that again ill stick to fixing the little things i run across as i read articles for my own information'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus200.iloc[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "_uuid": "f9c2329c87d297700c73a785020ee65961d6b609"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thanks josette i enjoyed meeting you too i was shocked by the decision which does not begin to reflect consensus does just one grand poobah make it alone serves me right for stealing time from more pressing realworld duties to indulge in a fun hobby ive learned my lesson and wont waste time like that again ill stick to fixing the little things i run across as i read articles for my own information'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus.iloc[113]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "31d8969d-df59-4795-a326-babe048c8bd6",
    "_uuid": "88db35f0b771134dc09e434fdb85141a52b2a4f2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##check the other language characters in the comment.\n",
    "ascii = set(string.printable)\n",
    "def remove_non_ascii(s):\n",
    "    return list(filter(lambda x: x not in ascii, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ea69d8cd-846f-4919-9bea-8b4aaaece71d",
    "_uuid": "52f067cdf22bf3919c2204abe5215545441d015b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# corp_ascii=corpus.apply(lambda x:remove_non_ascii(x))\n",
    "clean_corpus_ascii=clean_corpus.apply(lambda x:remove_non_ascii(x))\n",
    "# corp_ascii=corpus.apply(lambda x: x not in ascii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e811bd5d-a110-4508-8e16-196d938e8852",
    "_uuid": "9d22d9b4e264614353625993d51424f7ee51221b",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "4daccd21-a577-4fb7-81ac-72353bbac17e",
    "_uuid": "d6eb09c6c629342aebb2b84e2961c24b638b9c9b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k={}\n",
    "for i,j in enumerate(clean_corpus_ascii):\n",
    "    if (len(j)!=0):\n",
    "        k[i]=j\n",
    "#         k.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "78f49433-4e4f-4a80-8ba7-38002a951141",
    "_uuid": "8192e379eef10240d663f933ec1fd00f42bfe235",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p={}\n",
    "for i,j in k.items():\n",
    "    if len(j)>10:\n",
    "        p[i]=j\n",
    "        #print(len(i))\n",
    "        #p.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "dc27d1fa-db20-46e7-8138-33a688300a06",
    "_uuid": "c23e63531e2d56694b35bf409119f21d61b46724",
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##check the other language containing comments.\n",
    "# 8912 are contain non-ascii characrters\n",
    "len(list(k.values()))\n",
    "#1960 words contain other language text\n",
    "len(list(p.values()))\n",
    "list(p.keys())[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b5fa815b-69a3-44b4-8530-5b057b13c8c5",
    "_uuid": "ea5bc60b22a9282dc6e91d700a2acd4f89667121",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "clean_corpus.iloc[24515]\n",
    "clean_corpus.iloc[22254]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bddee06a-643c-47c6-b620-c83963a15b33",
    "_uuid": "659a0f5b8ed52052fcce4ccfdbbe4e355d83a8da",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['comment']=clean_corpus[:train.shape[0]]\n",
    "test['comment']=clean_corpus[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aad008cc-c7ff-4441-b71a-42ebddcb08e6",
    "_uuid": "710f8dc6fea07379e50ebcdb0325fc2673728878",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columnsTitles=['id', 'comment','toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']\n",
    "train1=train.reindex(columns=columnsTitles)\n",
    "train1.columns = ['id', 'comment_text','toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']\n",
    "train1.to_csv('train_clean.csv',encoding = \"utf8\", index=False)\n",
    "\n",
    "columnsTitles=['id', 'comment']\n",
    "test1=test.reindex(columns=columnsTitles)\n",
    "test1.columns=['id', 'comment_text']\n",
    "test1.to_csv('test_clean.csv',encoding = \"utf8\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
